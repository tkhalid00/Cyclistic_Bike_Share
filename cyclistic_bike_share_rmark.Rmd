---
title: "Cyclistic Bike Share - Analysis"
author: "tk"
date: "6/4/2021"
output:
  html_document:
    keep_md: true
    toc: true
    toc_depth: 2
---

The analysis is a part of a full project that I did for getting qualified for Google Data Analytics certificate. For the purpose, a huge dataset of approximately 500MB was provided. It was too large to be uploaded here. However, this public dataset can be accessed [here](https://divvy-tripdata.s3.amazonaws.com/index.html).

There are 12 `.csv` files for past 12 months, starting from April 2020 till March 2021. These raw data files can be downloaded for data analysis. Due to high volume of these files, I will only attach `.rdata` file format here.

We will jump right to our task with a systematic step-by-step approach.

# Intorduction

Cyclistic Bike Share company's marketing manager is convinced that out of following two categories of riders\
  
\* Casual riders, and  
\* Members
  
Members category is more profitable and it will lead to future growth of the company. For the purpose, marketing manger wants to get aware of major trends for these two classes in order to devise a comprehensive marketing strategy to shift more and more casual riders to members category. A junior data analyst's task is to identify these trends and communicate to the marketing manager in an effective way so that he can build an effective campaign.

# Step-1 - Ask - Define the Business Task

The business task at hand is to increase the number of riders in **Members** category. Following is the business task statement:  
>To find a comprehensive strategy to convert casual riders to members category in order to maximize the prospects of long term growth and profitability of the Cyclistics.

# Step-2 - Prepare - Check the Authenticity and Reliability of Data
In order assess the data, it needs to be looked at. Although, the tool for analysis will be decided in next stage, but before going there, we have to make sure that this data is reliable enough that it has the worth to be analyzed further. We need to check the data for following parameters:
1. Where is the data is located? Since the data is provided by the company itself, and no second party or third party is involved, there is a high probability that data is reliable.
2. How is data organized? Data is organized in `.csv` file format and named in pretty conventional and easy to understand way. name starts with year, followed by month number.
3. Data is reproducible for research purposes.
4. Only primary data sources have been used for this data.


# Step-3 - Process - Start Navigating Through Given Data
Now when we have assessed the quality and source of data and have it downloaded to a secure location, the next step is to make data ready for analysis. Although there are many tools that can help analyze the data, I have used R to explore, analyze and then visualize the given dataset. There are number of reasons for doing so.
1. Data set is too large to manipulate in Excel. While Excel is a wonderful tool to analyze and visualize small-to-medium sized data sets, it has its limitation when it comes to large data sets. Excel can easily be slowed down by any huge dataset. Current data set contains almost 4 million rows and for Excel its too much of a task to go through this rows quickly.
2. Another option is using SQL. No doubt that SQL is much faster than Excel in handling these large data sets, but it needs some external server where data needs to be uploaded and where queries can be run to extract and manipulate data. Due to large size of `.csv` files, it was a challenging task to upload these data files to some external environment like Google Cloud services. These external cloud are exceptionally good with data storage and query execution, but most of the times these services offer limited access with free accounts.
3. Finally, I decided to go with R, not because it's an open source programming platform, but offers great flexibility in terms of manipulating and visualizing data. It's kind of one stop shop where you can perform many task without incurring any cost.

In order to process data before analysis, the first task is to import data into R. To do so, a couple of packages needs to be installed/loaded. You can write the following code to load these libraries:

```{r Load-Libraries, message = FALSE}
library(tidyverse) # This package is a combination of different packages required for data wrangling, analysis and visualization.
  
library(lubridate) # This package deals with manipulation of dates.
  
library(scales) # This package helps simplify scales while drawing different graphs and charts.
  
library(cowplot) # This package is helpful if multiple plots are required to be produced or saved side by side.

library(Amelia) # This package is helpful to visualize NA values in a given data set.
```

Next we need to import data into R studio to have an initial look of the given data. Due to large size of `*.csv` files, this code will not be executed in following chunks, and, hence will be commented out.  

First step to import these files into R is to get all the names of `*.csv` data files in the folder where these files are stored. Since I used `data/unconditioned_data` folder to download and store these files, I have given the path and used `list.files()` to get the names of all `.csv` files in given folder.

```{r Create-list-of-csv-files}
# # get all the file names with their path in a variable ===========
# 
# list_of_files <- list.files("data/unconditioned_data", pattern = "*.csv", 
#                             full.names = TRUE)
# list_of_files
# class(list_of_files)
# length(list_of_files)
```

Before moving further, instead of importing all the files together, its a good practice that we select a single file from above created list and import to check its structure and get a feel of raw data. Again, due to high volume these `.csv` files, this code will not be executed here. We will start our analysis once we get all the data in a single file that is compatible with R and easy to use to manipulate and analyze data.

```{r check-file-structure}
# # before combining these files, we need to make sure that these files are being
# # combined in the right format. To ascertain the right file structure, import a 
# # a single file into R, and check the variables and define the variable type,
# # if required
# 
# check_file_stru <- read_csv(list_of_files[1])
# str(check_file_stru)
# glimpse(check_file_stru)
# colnames(check_file_stru)

```


As these `.csv` file names are stored as a list in R, the next step is to import these raw `.csv` files into R as as a data frame. As this data frame is still not in compatible R format that we need, this code will also not be executed here. In `read_csv()`, we use `col_types = `, the types of columns that we need in our desired data frame are defined at the time of data import. By doing this, we get all the data in a uniform format that is compatible with each other and will remove and errors in future due to data incompatibility. `col_types` abbreviations represent the following:
* c = `character`
* f = `factor`
* T = `dateTime` of class `POSIXct`
* d = `double`

```{r import-data-in-R-data-frame}
# # To Combine Data into a Data Frame =======
# 
# # use map_df function to match all the extracted data into a data frame variable
# # in map_df function, '~' is used to define a modified function
# # read_csv() is originally designed to read a single file, but '~' shows that
# # the function has been modified to read a whole list instead of reading a 
# # single file
# 
# tripdata_df <- list_of_files %>% 
#   map_df(~read_csv(.x, col_types = "cfTTccccddddf"))
# tripdata_df
# 
# dim(tripdata_df)
# 
# tripdata_df
```

After import of 12 no. of `.csv` into a single R compatible data frame vector, we need to save it in `.rdata` format. There are several advantages of doing so:
1. It takes less than one-fourth of the original file size.
2. `.rdata` files can be imported easily into `R` Environment and they can be loaded instantly without a requirement of doing any data format conversions.
Now, this master data will be saved as `.rdata` file

```{r save-as-rdata}
# # Save and .Rdata for saving data files ======
# 
# # Use "Save" and ".rdata" extension to reduce storage size of data file
# # efficient write function for csv but takes a fraction (1/4th) of space as compared
# # to write_csv function
# 

# save(tripdata_df, file = "data/master_data.rdata")

```

Finally, we are able to get our hands on a `.rdata` file that can be easily loaded into R environment, so from now on all the code chunks will be compiled and executed. First load `.rdata` file into the system.

```{r load-rdata-file}
# Another advantage of .rdata file is that it can be easily loaded into R
# environment within seconds . This file can be carried anywhere in any other 
# R project, without the need of carrying original bulk spaced .csv files.
# load the .rdata file

load("data/master_data.rdata")

glimpse(tripdata_df)

```

We just have to name the `.rdata` file in destination folder to load it by using `load()`, it will create its R variable itself and will be ready to use in seconds. `glimpse()` function is very useful member of `pillar` package and it literally provides an outlook/structure of the given object.  

Although, it's not a requirement, but personally I have found it to be a good practice if we do not operate on our master data frame for data wrangling and manipulation in order to avoid any accidental data over writing on our master data. Therefore, we are going to create a copy of our master data frame. Although, it will take some space but this additional space usage is worth it as this precautionary measure can save our master data from any accidental loss or damage.  

```{r master-data-set-copy}
# Make a copy of original dataset to avoid over writing original dataset

df2 <- tripdata_df


# make a new data frame with this information
save(df2, file = "data/df2.rdata")


## Load 'df2' dataset

load("data/df2.rdata")

class(df2)

```


Now when we have our data ready, we need to add some more variables into our data frame for getting this data ready for analysis. 
Since this data set is all about cycle trips, the most important parameter in this data set is the trip duration. We will introduce a new variable with the name of `trip_dur` which will store the total time of each trip. `%--%` is a trip duration calculation operator that is used calculate time when given start and end time in `POSIXct` format.

```{r tirp-calculator}
# Data wrangling ==============

# arrange data with respect to date and time

df2 <- df2 %>% 
  arrange(started_at)

df2

# introduce a new column variable that can calculate trip length
# use duration calculation operator to calculate the trip duration

trip_dur <- df2 %>% 
  with(started_at %--% ended_at) %>%
  as.duration() %>% 
  as.numeric("minutes") %>% 
  round(2)

# add this new useful variable to the data frame df2

df2 <- df2 %>% 
  mutate(trip_dur)

glimpse(df2)
summary(df2$trip_dur)
```

`smmmary()` of this new variable shows that there is at least one trip duration that is negative, i.e. less than zero. Since, its not possible to have a negative trip duration, so data set needs further investigation. For the purpose, a boxplot will be drawn.

```{r all-values-box-plot}
# draw a boxplot to have a clear view of scattered data
# draw a boxplot to have a clear view of trip duration 
ggplot(df2) +
  geom_boxplot(aes(x = trip_dur)) +
  labs(title = "Boxplot for Trip Duration", x = "Trip Duration (in minutes)") +
  scale_x_continuous(labels = scales::comma)

# save this boxplot
ggsave("fig_out/00_0_neg_values_trip_duration.png")

```

As we can see there are number of instances where negative trip duration has been recorded in data. In order to check total number of negative duration values, we will use `filter` to check exact number of these values and their spread.

```{r duration-negative-values}
neg_dur <- df2 %>% 
  filter(trip_dur < 0)
glimpse(neg_dur$trip_dur)
```

We can have a boxplot to check the spread of negative values in our given dataset.

```{r negative-duration-plot}
ggplot(neg_dur) +
  geom_boxplot(aes(x = trip_dur)) +
  labs(title = "Negative Trips", x = "Trip Duration (mins)") +
  scale_x_continuous(labels = scales::comma)
```

As the above mentioned boxplot shows that there are some values that are far away from possible minimum value of time duration i.e. 0 minutes. Let's examine the spread of these neagtive values:
```{r summary-neg-trips}
fivenum(neg_dur$trip_dur)
```
From above summary, we can see that although there are some extreme negative duration values, overall negative value set is skewed towards zero. Therefore, I could remove these negative values, as extreme values are impossible to have as negative trip duration and values close to zero doesn't impact much as there are only around 10,500 negative values out of more than 4 million entries. So I am going to remove these negative values. To do so, a new dataset will be created.

```{r new-dataset-filtering-neg-val}
df3 <- df2 %>% 
  filter(trip_dur >= 0 )

save(df3, file = "data/df3.rdata")

load("data/df3.rdata")
glimpse(df3)
```

We can have a quick summary of both data sets (before and after removal negative values).
```{r summary-neg-non-neg}
summary(df2$trip_dur)
summary(df3$trip_dur)
```

Above data confirms that while we successfully removed negative values, it didn't impact much overall data spread or mean. Now we can add some useful new variables (columns) to our data in order to prepare it for further investigation.
```{r add-new-vars-date-day}
# add a variable of type Date, it will make it easier to analyze data
df3$date <- as.Date(df3$started_at)

# add a month column
df3$month <- factor(format(df3$date, "%b"),
                    level = c("Jan", "Feb", "Mar", "Apr", "May", "Jun", "Jul",
                              "Aug", "Sep", "Oct", "Nov", "Dec"))

# add a weekday column
df3$day <- factor(format(df3$date, "%a"),
                  level = c("Mon", "Tue", "Wed", "Thu", "Fri", "Sat", "Sun"))

df3$start_time <- factor(format(df3$started_at, "%H:00"))
str(df3$start_time)

glimpse(df3)
```
Now when all the necessary columns have been added, we need to remove some fields that are not required at the moment for our data analysis. Let's get them removed.
```{r drop-unnecessary-columns}
# drop some columns that are not required for analysis

df3 <- df3[, -c(3, 4, 6, 8, 9, 10, 11, 12)]
colnames(df3$start_time)
```
As a final step of data processing stage, check for `NA` values in the data set. `Amelia` package is a convenient way to check for overall presence of `NA` values in a data set in a graphical interesting way.

```{r missing-values-check-Amelia}
# Check for NA Values in given data set. Use Amelia package to check for NA values in given data set

missmap(df3)

```
AS it can be seen in above graph, that there are around less than 1% values that are missing, and that too in `start_station_name` and `end_station_name`, these values are not going to affect our analysis much in terms of reliability, so we will leave these missing values untouched.  

we would like to save this data set in a new data set for compartmentalization and easy loading in future.
```{r}
#Create a new data set df4 for the data that is super ready for analysis
df4 <- df3
save(df4, file = "data/df4.rdata")
```






# Step-4 - Analysis and Visualising

Now coming to the Analysis part, we will plot several graphs for visualizing several trends and patterns for member vs. casual riders. First we need to load our data set which we saved at the end of last stage.
```{r load-d4-dataset}
load("data/df4.rdata")
```

It would be a logical  thing if we start analyzing from drawing total number of trips occurred on each day of the week.
```{r graph-trips-weekdays}
# Draw total number of trips w.r.t. weekdays and save the graph

ggplot(data = df4) +
  geom_bar(aes(x = day), fill = "light blue") +
  xlab("Weekdays") +
  ylab("No. of Trips") +
  ggtitle("Total no. of Trips wrt. Weekdays") +
  scale_y_continuous(labels = scales::comma)

ggsave("fig_out/01_weekdays_total_trips.png")
```

It's evident that weekends are busier than the working days so let's plot a member and non-member graph trip.
```{r member-non-member-graph}
# Trips by member and casual categories
ggplot(data = df4) +
  geom_bar(aes(x = day, fill = member_casual), position = "dodge") +
  labs(x = "Weekdays", y = "No. of Trips", 
       title = "Total no. of Trips wrt. Weekdays") +
  theme(legend.title = element_blank()) +
  scale_y_continuous(labels = scales::comma)

ggsave("fig_out/01_1_weekdays_total_trips.png")
```

Now, let see how these trips are translated in terms of toal numbers and percentage in both of these categories
```{r total-trips-and-percentage}
# Draw total number of trips in member and casual segregation - Percentage
plot_grid(
  ggplot(data = df4, aes(x = member_casual, fill = member_casual)) +
    geom_bar(aes(position = "dodge")) +
    labs(x = "Member Status", y = "no_of_trips", title = "Total Trip Segregation \nMember vs. Casual") +
    theme(legend.title = element_blank(), 
          legend.position = "none", 
          axis.text.x = element_text(angle = 0, vjust = 0.5, hjust = 0)) +
    scale_y_continuous(labels = scales::comma),
  ggplot(data = df4, aes(x = member_casual, fill = member_casual)) +
    geom_bar(aes(y = (..count..)/sum(..count..), position = "dodge")) +
    labs(x = "Member Status", y = "Percentage", title = " \n ") +
    theme(legend.title = element_blank(), 
          legend.position = "right", 
          axis.text.x = element_text(angle = 0, vjust = 0.5, hjust = 0))  +
    scale_y_continuous(labels = scales::percent)
)

ggsave("fig_out/03_no_of_trips_member_vs_casual.png")
```

As depicted by the graph, on weekends, number casual cyclists jump cross the member cyclists category. Now let's examine the trip duration of these cycle trips.
```{r trip-duration}
# Trip Duration Scatter plot
ggplot(df4) +
  geom_point(aes(x = day, y = trip_dur)) +
  xlab("Weekdays") +
  ylab("Trip Duration (in Mins.)") +
  ggtitle("Trip durations by Weekdays") +
  scale_y_continuous(labels = scales::comma)

ggsave("fig_out/02_trip_dur_wrt_weekdays_point_plot.png")
```
  
As most of the casual riders avail Cyclistics services on weekend, we can break down hourly traffic on weekends.
```{r weekend-hourly-traffic}
# Create a new data frame to filter Saturday and Sunday traffic

df5_sat_sun <- df4 %>% 
  filter(day == "Sat" | day == "Sun")

plot_grid(
  ggplot(data = df5_sat_sun, aes(x = day)) +
    geom_bar(aes(fill = member_casual), position = "dodge") +
    labs(x = "Weekdays", y = "No. of Trips", 
         title = "Total no. of Trips on\n Weekends") +
    theme(legend.title = element_blank(), legend.position = "left",
          axis.text.x = element_text(angle = 0, vjust = 0.5, hjust = 0)) +
    scale_y_continuous(labels = scales::comma),
  ggplot(df5_sat_sun, aes(x = start_time)) +
    geom_bar(aes(fill = member_casual), position = "dodge") +
    theme(legend.title = element_blank(), legend.position = "none",
          axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 0)) +
    labs(x = "time (hours)", y = "no. of trips", 
         title = "Trips - Round the Clock - On Weekends") +
    scale_y_continuous(labels = scales::comma)
)

ggsave("fig_out/05_sat_sun_trips.png")


# Side by Side Plot Weekdays vs. Weekends
plot_grid(
  ggplot(data = df4, aes(x = member_casual, fill = member_casual)) +
    geom_bar(aes(y = (..count..)/sum(..count..), position = "dodge")) +
    labs(x = "Member Status", y = "Percentage", title = "Total Trips \nAll Weekdays") +
    theme(legend.position = "none") +
    scale_y_continuous(labels = scales::percent),
  ggplot(data = df5_sat_sun, aes(x = member_casual, fill = member_casual)) +
    geom_bar(aes(y = (..count..)/sum(..count..), position = "dodge")) +
    labs(x = "Member Status", y = "", title = "Total Trips \nSaturdays & Sundays Only") +
    theme(legend.position = "none") +
    scale_y_continuous(labels = scales::percent)
)  

ggsave("fig_out/09_no_of_trips_weekdays_vs_weekend.png")

```

As shown in the graph, on weekends, casual riders comprise almost 50% of the total customers, as opposed to 40% of the whole week combined. Let's break down the these two days' trip in terms of timings to see if there are any patterns for peak hours.
```{r Sat-Sun-Peak-Patterns}
# Create hourly traffic plot for Saturdays and Sundays
ggplot(df5_sat_sun, aes(x = day)) +
  geom_bar(aes(fill = member_casual), position = "dodge") +
  labs(title = "Cycle Usage during the Day - Round the Clock", 
       x = "Time in Hours", y = "No. of Trips") +
  theme(legend.title = element_blank(), legend.position = "right",
        axis.text.x = element_blank()
        ) +
  scale_y_continuous(labels = scales::comma) +
  facet_grid(day ~ start_time)


ggsave("fig_out/06_sat_sun_trips_pattern.png", width = 11, height = 7, units = "in")
```

Both days have the same pattern as casual number of riders exceed member riders around 12 noon and this trend lasts till mid-night.

By observing the above patterns, following are my three observations:
1. Starting from Friday till Sunday, three days of the week are the busiest days of the week in which Saturdays exceeds all the other days.
2. Saturdays and Sundays have the highest trip duration.
3. Casual riders trips exceed regular member trips on weekends and account for more than 50% of the trips on these two days.
